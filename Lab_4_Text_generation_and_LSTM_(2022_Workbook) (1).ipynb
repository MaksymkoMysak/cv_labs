{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "c-tiVIhwDZgF"
   },
   "source": [
    "# Генерація тексту\n",
    "\n",
    "У цій роботі ми використаємо мовну модель для генерації тексту. Її бажано виконувати після перегляду [лекції 4.2](https://youtu.be/yAUCnoKW2QI)\n",
    "\n",
    "У класичної мовної моделі є два взаємопов'язані визначення:\n",
    "\n",
    "1. Оцінити ймовірність вхідного тексту.\n",
    "2. Маючи певний префікс на вході, видати ймовірностий розподіл наступного слова.\n",
    "\n",
    "Для генерації тексту нам ідеально підходить друге визначення."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "n5cdtYXg9ARX"
   },
   "source": [
    "## Початок роботи\n",
    "\n",
    "Будь ласка, заповніть поля `EMAIL`, `NAME` та `GROUP` нижче:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "vuKrzAhF9DS9"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'YES!'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#############################################№№№№№№№№###########################\n",
    "# FILL-IN:\n",
    "#-----------------------------------------------------------------------\n",
    "EMAIL = \"maksym.mysak.knm.2018@lpnu.ua\"    # наприклад, oleksiy.syvokon@lpnu.ua\n",
    "NAME = \"Максим Мисак\"            # наприклад, \"Олексій Сивоконь\"\n",
    "GROUP = \"КН-409\"  # підставте вашу групу, залиште КН-400, якщо жодна не підходить\n",
    "#####################################№№№№№№№№###################################\n",
    "\n",
    "import requests\n",
    "import json\n",
    "\n",
    "def report(stage, answer):\n",
    "    if answer is ...:\n",
    "        raise ValueError(\"Please, implement a solution\")\n",
    "\n",
    "    payload = {\"email\": EMAIL, \"name\": NAME, \"group\": GROUP}\n",
    "    payload[\"stage\"] = str(stage)\n",
    "    payload[\"answer\"] = str(answer)\n",
    "    payload[\"lab\"] = \"lab4\"\n",
    "    \n",
    "    r = requests.post(\"http://134.209.248.229:8082/report\", json=payload)\n",
    "    if not r.ok:\n",
    "        print(\"Проблема з сервером :( Спробуйте пізніше або напишіть викладачеві\")\n",
    "    \n",
    "    return answer\n",
    "\n",
    "assert EMAIL, \"Заповніть поле EMAIL\"\n",
    "assert NAME, \"Заповніть поле NAME\"\n",
    "report(\"Ready\", \"YES!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nIrY-_DL9XRv"
   },
   "source": [
    "### Завантаження моделі\n",
    "\n",
    "Тренування мовної моделі з нуля займає багато часу: від кількох годин до кількох днів для маленьких та середніх моделей й до кількох місяців чи навіть років для великих (звичайно, за рахунок .\n",
    "\n",
    "Для цієї роботи я попередньо натренував невеличку LSTM модель. Тренувальними даними були речення українською мовою, набрані з інтернету навмання. Текст був приведений до нижнього регістру. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "Oadzovl02gxh"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'wget' is not recognized as an internal or external command,\n",
      "operable program or batch file.\n"
     ]
    }
   ],
   "source": [
    "!wget http://134.209.248.229:8081/oscar-epoch3.pt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "T4su2VnEGSiU"
   },
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZpeVt8kn76PY"
   },
   "outputs": [],
   "source": [
    "model_state = torch.load(\"oscar-epoch3.pt\", map_location=torch.device('cpu'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uugdVo_l8BK7"
   },
   "outputs": [],
   "source": [
    "import math\n",
    "import random\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class LstmLM(nn.Module):\n",
    "\n",
    "    def __init__(self, vocab_size, dim_embed, dim_hidden, num_layers, dropout=0.5, tie_weights=False):\n",
    "        super().__init__()\n",
    "        self.vocab_size = vocab_size\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.embed = nn.Embedding(vocab_size, dim_embed)\n",
    "        self.rnn = nn.LSTM(dim_embed, dim_hidden, num_layers)\n",
    "        self.linear = nn.Linear(dim_hidden, vocab_size)\n",
    "\n",
    "        if tie_weights:\n",
    "            if dim_hidden != dim_embed:\n",
    "                raise ValueError('When using the tied flag, dim_hidden must be equal to dim_embed')\n",
    "            self.linear.weight = self.embed.weight\n",
    "\n",
    "        self.init_weights()\n",
    "\n",
    "        self.dim_hidden = dim_hidden\n",
    "        self.num_layers = num_layers\n",
    "\n",
    "    def init_weights(self):\n",
    "        initrange = 0.1\n",
    "        nn.init.uniform_(self.embed.weight, -initrange, initrange)\n",
    "        nn.init.zeros_(self.linear.weight)\n",
    "        nn.init.uniform_(self.linear.weight, -initrange, initrange)\n",
    "\n",
    "    def forward(self, input, hidden):\n",
    "        emb = self.dropout(self.embed(input))\n",
    "        output, hidden = self.rnn(emb, hidden)\n",
    "        output = self.dropout(output)\n",
    "        decoded = self.linear(output)\n",
    "        decoded = decoded.view(-1, self.vocab_size)\n",
    "        return F.log_softmax(decoded, dim=1), hidden\n",
    "\n",
    "    def init_hidden(self, bsz=1):\n",
    "        weight = next(self.parameters())\n",
    "        return (weight.new_zeros(self.num_layers, bsz, self.dim_hidden),\n",
    "                weight.new_zeros(self.num_layers, bsz, self.dim_hidden))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "H0NgmBJ89UvA"
   },
   "outputs": [],
   "source": [
    "model = LstmLM(vocab_size=32000,\n",
    "               dim_embed=512,\n",
    "               dim_hidden=512,\n",
    "               num_layers=3,\n",
    "               dropout=0.3,\n",
    "               tie_weights=True)\n",
    "model.load_state_dict(model_state)\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QNW6QMBXQf0Y"
   },
   "source": [
    "### Словник"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "s_hRjqHWOlIV"
   },
   "source": [
    "Завантажимо словник, з яким тренувалася модель:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kgLkneSq-0Ps"
   },
   "outputs": [],
   "source": [
    "!wget http://134.209.248.229:8081/bpe.dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "S-wYEN2JPrg8"
   },
   "source": [
    "Клас `Vocab` допоможе зручно виконувати такі часті операції:\n",
    "\n",
    "1. `Vocab.word2idx()` - отримати індекс слова\n",
    "2. `Vocab.idx2word()` - отримати слово за індексом"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lsyV4GVu-X30"
   },
   "outputs": [],
   "source": [
    "class Vocab(object):\n",
    "    def __init__(self):\n",
    "        self.word2idx = {}\n",
    "        self.idx2word = []\n",
    "\n",
    "    @classmethod\n",
    "    def from_file(cls, path):\n",
    "        d = Vocab()\n",
    "        with open(path) as f:\n",
    "            for word in f:\n",
    "                d.add_word(word.rstrip(\"\\n\"))\n",
    "        return d\n",
    "\n",
    "    def add_word(self, word):\n",
    "        if word not in self.word2idx:\n",
    "            self.idx2word.append(word)\n",
    "            self.word2idx[word] = len(self.idx2word) - 1\n",
    "        return self.word2idx[word]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.idx2word)\n",
    "\n",
    "vocab = Vocab.from_file(\"bpe.dict\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mIUF4cnFQP_v"
   },
   "outputs": [],
   "source": [
    "# Перевірка\n",
    "vocab.word2idx[\"the\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7WGu5ORJQYkm"
   },
   "outputs": [],
   "source": [
    "vocab.idx2word[22586]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "S3Uh8W7wMpwE"
   },
   "source": [
    "## Перевірка моделі -- один крок ітерації"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_LiBLRgyNu32"
   },
   "source": [
    "Ми генеруватимемо текст в циклі токен за токеном, зліва направо.Але для початку розберемо, як виглядає один крок такого циклу. \n",
    "\n",
    "Рекурентні мережі зберігають свою \"пам'ять\" або \"стан\" у векторі (у випадку з LSTM, двох векторах). Цей стан містить в собі інформацію про вже побачені токени. \n",
    "\n",
    "Оскільки на першому кроці ми ще бачили жодного токену, ініціалізуємо \"пам'ять\" нулями та збережемо її у змінній `hidden`:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "EN12UkTcM5ub"
   },
   "outputs": [],
   "source": [
    "hidden = model.init_hidden()\n",
    "hidden"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "c9cbLyy3N-u9"
   },
   "source": [
    "На кожному кроці на вхід моделі подаємо наступний токен речення. Починаємо зі спеціального токену `<BOS>` (\"begin of sentence\"):\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CjeJgPqtObeQ"
   },
   "outputs": [],
   "source": [
    "# Знайдемо індекс токена в словнику\n",
    "index = vocab.word2idx[\"<BOS>\"]\n",
    "index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "y6z8l1TlRQa-"
   },
   "outputs": [],
   "source": [
    "# Формуємо вхідний батч. Майже завжди для більшої ефективності\n",
    "# нейронні мережі очікують на вхід кілька незалежних речень \n",
    "# (або зображень у випадку з комп'ютерним зором)\n",
    "#\n",
    "# В нашій роботі ми завжди працюємо лише з одним реченням,\n",
    "# тож розмір батча дорівнює одинці. Але все одно маємо\n",
    "# оформити вхід як матрицю:\n",
    "input_ = torch.LongTensor([[index]])\n",
    "input_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vSiBQkJJSGue"
   },
   "outputs": [],
   "source": [
    "# Нарешті робимо крок LSTM\n",
    "# Вхід: поточний стан (пам'ять) та вхідний токен\n",
    "# Вихід: розподіл по словнику (передбачення наступного токена) та оновлений стан\n",
    "output, hidden = model(input_, hidden)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xNoTYS-1Sr2y"
   },
   "source": [
    "Перевіримо ймовірностний розподіл, який видала модель:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dMCovI7uS1N8"
   },
   "outputs": [],
   "source": [
    "# Розмірність збігається з розміром словника\n",
    "output.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_1H8iIaSTHJl"
   },
   "outputs": [],
   "source": [
    "# Модель повертає логарифми ймовірностей\n",
    "# Якщо ми проекспоненціонуємо їх, отримаємо \"звичайні\" ймовірності\n",
    "probs = output.squeeze().exp()\n",
    "\n",
    "# Сума ймовірностей має дорівнювати 1.0\n",
    "probs.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kA1iznWPTdK4"
   },
   "outputs": [],
   "source": [
    "# Кожному слову в словнику відповідає своя ймовірність бути побаченим\n",
    "# після заданого префікса. Префіксом у нас зараз був лише одни токен <BOS>\n",
    "\n",
    "# Яка ймовірність, що речення почнеться зі слова я?\n",
    "probs[vocab.word2idx[\"я\"]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kaoCr91RTw4Z"
   },
   "source": [
    "Тепер, коли маємо ймовірностний розподіл по словнику, можемо обрати слово, яке вважатимемо згенерованим. Тут можливі кілька стратегій, які ми розглянемо в наступних розділах."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wUbajoW_-ClA"
   },
   "source": [
    "## Greedy decoding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9yOkcB3FUd3N"
   },
   "source": [
    "Найпростіший (але й не дуже цікавий) спосіб -- це завжди обирати токен з найбільшою ймовірністю:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wG7QsAjPUoHW"
   },
   "outputs": [],
   "source": [
    "next_token_id = probs.argmax()\n",
    "next_token_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Z87pe4Y0VJ1W"
   },
   "outputs": [],
   "source": [
    "probs[100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WaZRZWpAUtNY"
   },
   "outputs": [],
   "source": [
    "vocab.idx2word[next_token_id]  # Наш перший згенерований токен"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "L5a6IGTg-x8e"
   },
   "outputs": [],
   "source": [
    "report(\"First generated token\", vocab.idx2word[next_token_id])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YVB5pXOoX29S"
   },
   "source": [
    "Зберемо наш код докупи та додамо цикл. В циклі ми продовжуватимемо генерувати текст токен за токеном, поки не настане одна з двох умов:\n",
    "1. Модель видала спецальний токен `<EOS>` (end of sentence)\n",
    "2. Довжина згенерованого тексту перевищила певний поріг `max_len`\n",
    "\n",
    "У хорошої моделі в більшості випадків має спрацьовувати перша умова зупнки. Проте іноді модель може впасти в безкінчений цикл. Щоб цьому запобігти, маємо другу умову."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cG8nI5WG-Qb5"
   },
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def greedy_decode(model, vocab, max_len=50):\n",
    "    result = []\n",
    "    EOS_TOKEN = \"<EOS>\"\n",
    "    hidden = model.init_hidden()\n",
    "    start_index = vocab.word2idx[\"<BOS>\"]\n",
    "    input_ = torch.LongTensor([[start_index]])\n",
    "\n",
    "    while len(result) < max_len:\n",
    "\n",
    "        # Передбачення ймовірностней наступного токена\n",
    "        output, hidden = model(input_, hidden)\n",
    "        probs = output.squeeze().exp()\n",
    "        \n",
    "        # Обираємо токен, що має найбільшу ймовірність\n",
    "        token_index = probs.argmax()\n",
    "\n",
    "        # Обраний токен стає наступним вхідним токеном для моделі\n",
    "        input_.fill_(token_index)\n",
    "        \n",
    "        # Додаємо обраний токен в згенерований текст\n",
    "        token = vocab.idx2word[token_index]\n",
    "        if token == EOS_TOKEN:\n",
    "            break\n",
    "        result.append(token)\n",
    "        \n",
    "    return \"\".join(result)\n",
    "\n",
    "\n",
    "greedy_decode(model, vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Qv0i3pqdJ3Bn"
   },
   "outputs": [],
   "source": [
    "report(\"greedy decode\", greedy_decode(model, vocab))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9GN1tc5kYsOk"
   },
   "source": [
    "### Примітка: Byte-pair encoding (BPE)\n",
    "\n",
    "Наша модель використовує subword токенізацію, а саме byte-pair encoding (BPE). В сучасному NLP це найрозповсюдженіший спосіб токенізації. Детально можете подивитися в [цьому відео](https://www.youtube.com/watch?v=tOMjTCO0htA).\n",
    "\n",
    "Для наших цілей зараз важливо, що BPE заміняє пробіли на спеціальні Unicode-символи \"▁\" (зверніть увагу, це не звичайний символ підкреслення \"_\"). Щоб отримати чистий текст, треба виконати наступну заміну:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "5S_BPCLY_V9h"
   },
   "outputs": [],
   "source": [
    "def bpe_decode(s):\n",
    "    result = s.replace(\"▁\", \" \")\n",
    "    if result.startswith(\" \"):\n",
    "        result = result[1:]\n",
    "    return result\n",
    "\n",
    "bpe_decode(greedy_decode(model, vocab))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vK0wKs4IAk4l"
   },
   "source": [
    "## Generic decoding function\n",
    "\n",
    "Обирати слово з найбільшою ймовірністю -- не найкращий варіант для генерації тексту хоча б тому, що він завжди детерміновано призводить до однієї послідовності. Нижче ми подивимося на цікавіші альтернативи.\n",
    "\n",
    "Цикл генерації залишиться той самий, що і в `greedy_decode()`. Відрізнятися буде лише один рядок -- той, в якому ми приймали рішення, яке слово обрати. Для зручності, винесемо цей рядок в окрему функцію. Ця функція прийматиме на вхід ймовірностний розподіл по словнику і повертає обраний токен. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8KQ_8Mky9enZ"
   },
   "outputs": [],
   "source": [
    "def greedy_choice(probs):\n",
    "    return probs.argmax()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XmH11o729oQL"
   },
   "source": [
    "Функцію генерації також трохи переробимо.\n",
    "\n",
    "По-перше, додамо параметр `sample_fn` -- це має бути функція, яка обирає слово з ймовірностного розподілу, наприклад, `greedy_choice`.\n",
    "\n",
    "По-друге, для зручності виконуватимемо BPE декодінг в середині функції генерації."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7eEGscC5Amzw"
   },
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def generate(model, vocab, sample_fn, max_len=50, bpe_decoded=True):\n",
    "    result = []\n",
    "    EOS_TOKEN = \"<EOS>\"\n",
    "    hidden = model.init_hidden()\n",
    "    start_index = vocab.word2idx[\"<BOS>\"]\n",
    "    input_ = torch.LongTensor([[start_index]])\n",
    "    \n",
    "    while len(result) < max_len:\n",
    "\n",
    "        # Передбачення ймовірностней наступного токена\n",
    "        output, hidden = model(input_, hidden)\n",
    "        probs = output.squeeze().exp()\n",
    "        \n",
    "        # Обираємо наступний токен\n",
    "        token_index = sample_fn(probs)     # <---------------- цей рядок змінено\n",
    "\n",
    "        # Обраний токен стає наступним вхідним токеном для моделі\n",
    "        input_.fill_(token_index)\n",
    "        \n",
    "        # Додаємо обраний токен в згенерований текст\n",
    "        token = vocab.idx2word[token_index]\n",
    "        if token == EOS_TOKEN:\n",
    "            break\n",
    "        result.append(token)\n",
    "\n",
    "\n",
    "    result_str = \"\".join(result)            # <---------------- ці рядки змінено\n",
    "    if bpe_decoded:\n",
    "        result_str = bpe_decode(result_str)\n",
    "\n",
    "    return result_str\n",
    "\n",
    "\n",
    "# Перевірка:\n",
    "generate(model, vocab, greedy_choice)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GXHiIrfZAnpW"
   },
   "source": [
    "## Simple sampling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7bbdLT96Bd8c"
   },
   "source": [
    "Перший альтернатива -- це sampling. Тут ми обираємо наступний токен випадково, але з урахуванням ймовірностей."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "01Y-djIkAsGB"
   },
   "outputs": [],
   "source": [
    "def simple_sample(probs):\n",
    "    return torch.multinomial(probs, num_samples=1)[0]\n",
    "\n",
    "# Згенеруємо 10 речень\n",
    "for i in range(1, 11):\n",
    "    result = generate(model, vocab, simple_sample, allow_unk=False)\n",
    "    print(f\"#{i}: {result}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tsFQnhgBEyGH"
   },
   "source": [
    "## Поліпшення семплінгу\n",
    "\n",
    "### Заборона `<UNK>`\n",
    "\n",
    "Іноді наша модель генерує `<UNK>` токени. Такі токени важливі в деяких випадках.\n",
    "\n",
    "Наприклад, візьмемо модель машинного перекладу, яка перекладає з української на англійську. На вхід моделі приходить речення:\n",
    "\n",
    "```\n",
    "Мене звати Олексій Сивоконь.\n",
    "```\n",
    "\n",
    "Якщо в словнику моделі немає слова \"Сивоконь\", то модель може згенерувати токен `<UNK>`. Це сигнал, що певні слова вона не може перекласти. В такому випадку вихід моделі буде виглядати якось так:\n",
    "\n",
    "```\n",
    "My name is Oleksiy <UNK>.\n",
    "```\n",
    "\n",
    "В такому випадку, як правило, запускають додатковий postprocessing модуль, який, скажімо, копіює невідомі слова з вхідного тексту:\n",
    "\n",
    "```\n",
    "My name is Oleksiy Сивоконь.\n",
    "```\n",
    "\n",
    "Не ідеально, але краще ніж нічого.\n",
    "\n",
    "Трохи розумніша система могла б робити транслітерацію.\n",
    "\n",
    "Отже, `<UNK>` токени важливі в машинному перекладі. Однак вони недоцільні у вільній генерації тексту. Тому ми просто заборонимо їх генерацію, призначивши їм нульову ймовірність."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XwJG_jGmBXCU"
   },
   "source": [
    "## Sampling with temperature\n",
    "\n",
    "Ми також можемо впливати на генерацію параметром температури softmax.\n",
    "\n",
    "Більші значення температури призводять до того, що різниця між ймовірностями токенів зменшується, тобто розподіл стає більш рівномірним. На практиці це означає, що менш ймовірні варіанти обиратимуться частіше і згенерований текст може бути цікавішим. Однак якщо продовжувати піднімати температуру, то текст спочатку втратить зв'язність, далі почнуть розпадатися слова та граматичність.\n",
    "\n",
    "Менші значення температури змінюють розподіл таким чином, що основна ймовірніста маса припадає на невелику кількість топових токенів. При температурі 0 вся ймовірність дістанеться одному токену й семплінг перетвориться на greedy decoding.\n",
    "\n",
    "Додамо в функцію `generate()` параметр `temperature` та згенеруємо тексти з різною температурою:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7meVwePeAsiu"
   },
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def generate(model, vocab, sample_fn, max_len=50, min_len=0, allow_unk=True, temperature=1.0):\n",
    "    result = []\n",
    "    hidden = model.init_hidden(1)\n",
    "    # input_ = torch.LongTensor([[random.randint(0, len(vocab))]])\n",
    "    input_ = torch.LongTensor([[vocab.word2idx[\"<BOS>\"]]])\n",
    "    \n",
    "    # Generate continuation\n",
    "    for _ in range(max_len):\n",
    "        output, hidden = model(input_, hidden)\n",
    "        probs = output[-1].squeeze().div(temperature).exp()   # TODO: is this correct?\n",
    "        \n",
    "        if not allow_unk:\n",
    "            probs[vocab.word2idx[\"<UNK>\"]] = 0.\n",
    "            \n",
    "        if len(result) < min_len:\n",
    "            probs[vocab.word2idx[\"<EOS>\"]] = 0.\n",
    "        \n",
    "        token_index = sample_fn(probs)\n",
    "        input_.fill_(token_index)\n",
    "        \n",
    "        token = vocab.idx2word[token_index]\n",
    "        if token == \"<EOS>\":\n",
    "            break\n",
    "        result.append(token)\n",
    "        \n",
    "    return bpe_decode(\"\".join(result))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0Y_J1eS3BfTF"
   },
   "outputs": [],
   "source": [
    "for temperature in (0.1, 0.3, 0.5, 0.8, 1.0, 1.5, 2.0, 3.0, 5.0):\n",
    "    print(f\"Sampling with temperature={temperature}\")\n",
    "    result = generate(model, vocab, simple_sample, temperature=temperature, allow_unk=False)\n",
    "    print(result)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ShTm-dddw6s9"
   },
   "outputs": [],
   "source": [
    "# Яке значення `temperature` здається вам оптимальною?\n",
    "report(\"best_temperature\", ...)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KO9-xZxRBkIB"
   },
   "source": [
    "## Top-k sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "HqPZTXxZCQdc"
   },
   "outputs": [],
   "source": [
    "def top_k_sampling(probs, k):\n",
    "    topk = probs.topk(k)\n",
    "    index = torch.multinomial(topk.values, num_samples=1)[0]\n",
    "#     print(f\"Top {k} words take {topk.values.sum():%} probability mass\")\n",
    "    return topk.indices[index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Q2BAZ9mnCRGC"
   },
   "outputs": [],
   "source": [
    "k = 15\n",
    "sample_fn = lambda probs: top_k_sampling(probs, k=k)\n",
    "for i in range(1, 10):\n",
    "    result = generate(model, vocab, sample_fn, allow_unk=False)\n",
    "    print(f\"#{i}: {result}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CuJFHRsaCoAK"
   },
   "source": [
    "## Nucleus (top-p) sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PUIZQgXhCU1l"
   },
   "outputs": [],
   "source": [
    "def nucleus_sampling(probs, max_p):\n",
    "    sorted_probs = probs.sort(descending=True)\n",
    "    cum_prob = 0.0\n",
    "    sample_indices = []\n",
    "    sample_probs = []\n",
    "    for i in range(0, len(sorted_probs.values)):\n",
    "        p = sorted_probs.values[i]\n",
    "        cum_prob += p\n",
    "        sample_probs.append(p)\n",
    "        sample_indices.append(sorted_probs.indices[i])\n",
    "        if cum_prob >= max_p:\n",
    "            break\n",
    "\n",
    "    index = torch.multinomial(torch.tensor(sample_probs), num_samples=1)\n",
    "    return sample_indices[index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FXuy2S8tCq_Z"
   },
   "outputs": [],
   "source": [
    "for max_p in (0.1, 0.3, 0.5, 0.6, 0.8, 1.0):\n",
    "    sample_fn = lambda probs: nucleus_sampling(probs, max_p=max_p)\n",
    "    result = generate(model, vocab, sample_fn, allow_unk=False)\n",
    "    print(f\"#{max_p}: {result}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UJCQ6ZrivApx"
   },
   "outputs": [],
   "source": [
    "# Яке значення p здається вам оптимальним?\n",
    "report(\"best_p\", ...)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ra-F6HyTCxcG"
   },
   "source": [
    "## Start from prompt\n",
    "\n",
    "До цього моменту ми генерували текст з нуля. Однак значно кориснішим є задача генерації тексту від певного префікса або \"підказки\" -- в англійській мові це називається \"prompt\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fccaVJA0DXPA"
   },
   "outputs": [],
   "source": [
    "!pip install youtokentome\n",
    "!wget http://134.209.248.229:8081/bpe.model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dSDiAvTwDc-t"
   },
   "outputs": [],
   "source": [
    "import youtokentome\n",
    "bpe = youtokentome.BPE(\"bpe.model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7mP3YWPfDI7Q"
   },
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def generate(model, vocab, sample_fn, max_len=50, temperature=1.0, allow_unk=True, prompt=None):\n",
    "    if prompt is None:\n",
    "        prompt = random.choice(vocab.idx2word)\n",
    "    prompt = [vocab.idx2word[x] for x in bpe.encode(prompt)]\n",
    "        \n",
    "    # force decode prompt\n",
    "    result = []\n",
    "    hidden = model.init_hidden(1)\n",
    "    for i, token in enumerate(prompt[:-1]):\n",
    "        index = vocab.word2idx[token]\n",
    "        input_ = torch.LongTensor([[index]])\n",
    "        output, hidden = model(input_, hidden)\n",
    "        \n",
    "    input_ = torch.LongTensor([[vocab.word2idx[prompt[-1]]]])\n",
    "    result = prompt[:]\n",
    "    \n",
    "    # Generate continuation\n",
    "    for _ in range(max_len):\n",
    "        output, hidden = model(input_, hidden)\n",
    "        probs = output[-1].squeeze().div(temperature).exp()\n",
    "        \n",
    "        if not allow_unk:\n",
    "            probs[vocab.word2idx[\"<UNK>\"]] = 0.\n",
    "        \n",
    "        token_index = sample_fn(probs)\n",
    "        input_.fill_(token_index)\n",
    "        \n",
    "        token = vocab.idx2word[token_index]\n",
    "        if token == \"<EOS>\":\n",
    "            break\n",
    "        result.append(token)\n",
    "        \n",
    "    return bpe_decode(\"\".join(result))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tBf2m-x7DJ0r"
   },
   "outputs": [],
   "source": [
    "k = 20\n",
    "sample_fn = lambda probs: top_k_sampling(probs, k=k)\n",
    "for i in range(1, 5):\n",
    "    # президент україни\n",
    "    # така несподівана заява\n",
    "    result = generate(model, vocab, sample_fn, allow_unk=False, prompt=\"магазини відкрили свої\")\n",
    "    print(f\"#{i}: {result}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0qBd5PJlxP2i"
   },
   "source": [
    "Спробуйте продовжити інші префікси:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MKCCCklFxF1a"
   },
   "outputs": [],
   "source": [
    "report(\"prompt1\", generate(model, vocab, sample_fn, allow_unk=False, prompt=\"студенти зустрілися з\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tiw3rFWuxOm4"
   },
   "outputs": [],
   "source": [
    "report(\"prompt2\", generate(model, vocab, sample_fn, allow_unk=False, prompt=\"президент україни\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PCWIBDUhxY7D"
   },
   "outputs": [],
   "source": [
    "report(\"prompt3\", generate(model, vocab, sample_fn, allow_unk=False, prompt=\"така несподівана заява\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wCrC6qPfxgq2"
   },
   "source": [
    "Придумайте свій початок речення:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "G7uJepzMxcWo"
   },
   "outputs": [],
   "source": [
    "report(\"prompt3\", generate(model, vocab, sample_fn, allow_unk=False, prompt=\"Все буде добре\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mslGz69EDO7z"
   },
   "source": [
    "# Autocompletion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0CsfNrIaHUPY"
   },
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def autocomplete(model, vocab, tokens, n=10):\n",
    "    if isinstance(tokens, str):\n",
    "        tokens = [vocab.idx2word[x] for x in bpe.encode(tokens)]\n",
    "\n",
    "    # force decode prompt\n",
    "    result = []\n",
    "    hidden = model.init_hidden(1)\n",
    "    for i, token in enumerate(tokens):\n",
    "        index = vocab.word2idx[token]\n",
    "        input_ = torch.LongTensor([[index]])\n",
    "        output, hidden = model(input_, hidden)\n",
    "    \n",
    "    topk = output.squeeze().topk(n)\n",
    "    words = [vocab.idx2word[x.item()] for x in topk.indices]\n",
    "    probs = [math.exp(p) for p in topk.values]\n",
    "    return list(zip(words, probs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qeGddb44HhKs"
   },
   "outputs": [],
   "source": [
    "autocomplete(model, vocab, \"сьогодні я побачив кілька\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "F4mGy8OSI7h0"
   },
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def score_sentence(model, vocab, tokens):\n",
    "    if isinstance(tokens, str):\n",
    "        tokens = [\"<BOS>\"] + [vocab.idx2word[x] for x in bpe.encode(tokens)]\n",
    "        \n",
    "    probs = []\n",
    "\n",
    "    # force decode prompt\n",
    "    result = []\n",
    "    hidden = model.init_hidden(1)\n",
    "    for i, token in enumerate(tokens):\n",
    "        index = vocab.word2idx[token]\n",
    "        input_ = torch.LongTensor([[index]])\n",
    "        output, hidden = model(input_, hidden)\n",
    "        log_prob = output[-1].squeeze()[index]\n",
    "        print(f\"{token:<20} {log_prob.item()}\")\n",
    "        probs.append(log_prob.item())\n",
    "        \n",
    "    return sum(probs) / len(probs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cemwEiF1H0Cw"
   },
   "source": [
    "# Score sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Fe6serIYI8Nk"
   },
   "outputs": [],
   "source": [
    "score_sentence(model, vocab, \"президент україни борщ\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2RGqKM9TJBQx"
   },
   "outputs": [],
   "source": [
    "score_sentence(model, vocab, \"президент україни заявив\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cuK8D9uQJR5c"
   },
   "outputs": [],
   "source": [
    "report(\"ALL DONE\", \"💪\")"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Lab 4 - Text generation and LSTM (2022 Workbook)",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
